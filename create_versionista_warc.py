from collections import Counter
from datetime import timezone
import email.utils
import hashlib
from http import HTTPStatus
import importlib.metadata
from io import BytesIO
from itertools import islice
import logging
from textwrap import dedent
import httpx
from tqdm import tqdm
from warcio import WARCWriter, StatusAndHeaders
from warcio.recordloader import ArcWarcRecord
import web_monitoring_db


logger = logging.getLogger()

# Headers known to have been generated by Versionista, and not sourced from the
# original captured request.
BAD_HEADERS = set([
    'age', 'date', 'vary', 'expires', 'x-cachee', 'connection', 'accept-ranges', 'cache-control', 'transfer-encoding'
])

GIGABYTE = 1024 * 1024 * 1024

WARC_VERSION = '1.1'


class BadDataError(Exception):
    reason = 'Bad_data'
    version_id = ''

    def __init__(self, version_id, message=None, reason=None):
        self.version_id = version_id
        self.reason = reason or self.reason
        super().__init__(f'{message or self.reason} (version={version_id})')


class MissingBodyError(BadDataError):
    reason = 'Body_never_saved'


# Based on warcio's implementation:
# https://github.com/webrecorder/warcio/blob/6775fb9ea3505db144a145c5a8b3ba1dfb822ac1/warcio/recordbuilder.py#L46-L52
# This is... not really up-to-spec, but generally good enough.
def serialize_warc_fields(fields_dict: dict) -> BytesIO:
    output = BytesIO()
    for name, value in fields_dict.items():
        if not value:
            continue

        line = f'{name}: {value}\r\n'
        output.write(line.encode('utf-8'))

    output.seek(0)
    return output


def create_metadata_record(writer: WARCWriter, uri: str, header: dict, data: dict) -> ArcWarcRecord:
    payload = serialize_warc_fields(data)
    return writer.create_warc_record(
        uri,
        'metadata',
        warc_headers_dict=header,
        payload=payload,
        length=payload.tell()
    )


def format_datetime_http(time):
    # email.utils does not support the UTC object dateutil uses, so fix it.
    return email.utils.format_datetime(time.astimezone(timezone.utc), usegmt=True)


def format_datetime_iso(time):
    iso_time = time.isoformat()
    # Use compact representation for UTC
    if iso_time.endswith('+00:00'):
        no_tz_date = iso_time.split("+", 1)[0]
        iso_time = f'{no_tz_date}Z'
    return iso_time


def status_text(code):
    status = HTTPStatus(code)
    return f'{status.value} {status.phrase}'


def load_response_body(version):
    body_response = httpx.get(version['body_url'])
    if body_response.status_code == 404:
        raise MissingBodyError(version['uuid'])

    actual_hash = hashlib.sha256(body_response.content).hexdigest()
    if actual_hash != version['body_hash']:
        detail = f'  Expected: {version["body_hash"]}\n  Actual:   {actual_hash}'
        raise AssertionError(f'Saved body does not match expected hash for version {version["uuid"]}\n{detail}')

    return body_response.content


revisit_cache: dict[str, dict] = {}


def create_version_records(warc: WARCWriter, version: dict) -> list[ArcWarcRecord]:
    records = []
    version_id = version['uuid']
    capture_time = version["capture_time"]

    if version['status'] is None:
        raise BadDataError(version_id, reason='Missing_status')

    if version['body_url'] is None:
        raise MissingBodyError(version_id)

    history = [version['url']]
    if version['source_metadata'].get('redirects'):
        history.extend(version['source_metadata']['redirects'])

    first_record_id = None
    previous_url = None
    final_url = history[-1]
    for index, url in enumerate(history):
        database_url = f'https://api.monitoring.envirodatagov.org/api/v0/versions/{version_id}'
        record_id = f'<{database_url}/responses/{index}>'
        warc_header = {
            'WARC-Record-ID': record_id,
            'WARC-Date': format_datetime_iso(capture_time),
            # This field is non-standard, and comes from warcit:
            #   https://github.com/webrecorder/warcit#warc-structure-and-format
            # We put the Versionista URL here and the WM database URL in the
            # metadata record.
            'WARC-Source-URI': version['source_metadata']['url'],
        }
        if index == 0:
            first_record_id = record_id
        else:
            warc_header['WARC-Concurrent-To'] = first_record_id

        recorded_headers = { 'Date': format_datetime_http(capture_time) }
        if url == final_url:
            if version['media_type']:
                recorded_headers['Content-Type'] = version['media_type']
            if version['headers']:
                for key, value in version['headers'].items():
                    if key.lower() not in BAD_HEADERS:
                        recorded_headers[key] = value
            http_headers = StatusAndHeaders(status_text(version['status']), recorded_headers.items(), protocol='HTTP/1.1')

            if version['body_hash'] in revisit_cache:
                revisit_data = revisit_cache[version['body_hash']]

                # For some reason this is not an option on create_revisit_record.
                warc_header['WARC-Refers-To'] = revisit_data['id']
                records.append(warc.create_revisit_record(
                    url,
                    revisit_data['warc_digest'],
                    revisit_data['uri'],
                    revisit_data['date'],
                    http_headers=http_headers,
                    warc_headers_dict=warc_header
                ))
            else:
                record = warc.create_warc_record(
                    url,
                    'response',
                    payload=BytesIO(load_response_body(version)),
                    http_headers=http_headers,
                    warc_headers_dict=warc_header
                )
                records.append(record)
                revisit_cache[version['body_hash']] = {
                    'id': record_id,
                    'warc_digest': record.rec_headers.get_header('WARC-Payload-Digest'),
                    'uri': url,
                    'date': warc_header['WARC-Date'],
                }
        else:
            recorded_headers['Location'] = history[index + 1]
            http_headers = StatusAndHeaders(status_text(302), recorded_headers.items(), protocol='HTTP/1.1')
            records.append(warc.create_warc_record(
                url,
                'response',
                payload=None,
                http_headers=http_headers,
                warc_headers_dict=warc_header
            ))

        if not first_record_id:
            first_record_id = record_id

        records.append(create_metadata_record(
            warc,
            url,
            header={
                'WARC-Date': format_datetime_iso(capture_time),
                'WARC-Refers-To': record_id,
                'WARC-Concurrent-To': first_record_id,
            },
            data={
                # Directly listed in WARC standard:
                'via': previous_url,
                'hopsFromSeed': 'R' * index,
                # Standardized via Dublin Core:
                'title': version['title'],
                'source': database_url
            }
        ))

        previous_url = url

    return records


# FIXME: implement rolling file writing based on `warc_size`.
def main(*, start=0, limit=0, name='versionista', gzip=True, warc_size=8 * GIGABYTE):
    logging.basicConfig(level=logging.WARNING)

    # The magic number here is the current count of Versionista records.
    expected_records = min(845_325, limit)

    chunk_size = min(1000, limit)
    db_client = web_monitoring_db.Client.from_env()

    skipped = Counter()

    filename = f'{name}.warc'
    if gzip:
        filename += '.gz'

    try:
        with open(filename, 'wb') as fh:
            writer = WARCWriter(fh, gzip=gzip, warc_version=WARC_VERSION)

            record = writer.create_warcinfo_record(filename, {
                # Or `import pkg_resources; pkg_resources.get_distribution('warcio').version
                'software': f'warcio/{importlib.metadata.version("warcio")}',
                'format': f'WARC file version {WARC_VERSION}',
                'operator': '"Environmental Data & Governance Initiative" <contact@envirodatagov.org>',
                'description': dedent("""\
                    Web content captured by EDGI's Web Monitoring project using
                    Versionista (https://versionista.com). This WARC is synthesized
                    from data that was originally archived extracted from
                    Versionista via https://github.com/edgi-govdata-archiving/versionista-outputter
                    and https://github.com/edgi-govdata-archiving/web-monitoring-versionista-scraper.""").replace('\n', ' ')
            })
            writer.write_record(record)

            versions = db_client.get_versions(source_type='versionista', chunk_size=chunk_size)
            if limit:
                versions = islice(versions, start, limit)

            progress_bar = tqdm(versions, unit=' versions', total=expected_records)
            for version in progress_bar:
                try:
                    for record in create_version_records(writer, version):
                        writer.write_record(record)
                except BadDataError as error:
                    progress_bar.write(f'WARNING: {error}')
                    skipped[error.reason] += 1
                except Exception as error:
                    raise RuntimeError(f'Error processing version {version.get("uuid")}: {error}') from error

        # FIXME: Add resource record with logs:
        # https://iipc.github.io/warc-specifications/guidelines/warc-implementation-guidelines/#use-of-resource-records-for-processing-information

    finally:
        print(f'Skipped {skipped.total()} Versionista versions:')
        for reason, count in skipped.items():
            print(f'  {reason.ljust(25, ".")} {str(count).rjust(5)}')


if __name__ == '__main__':
    main(limit=10, name='edgi_wm_versionista', gzip=False)
