from collections import Counter, OrderedDict
from datetime import timezone
import email.utils
import hashlib
from http import HTTPStatus
import importlib.metadata
from io import BytesIO, BufferedWriter
from itertools import islice
import logging
from pathlib import Path
from textwrap import dedent
from typing import Self
from dateutil.parser import parse as parse_timestamp
import httpx
from tqdm import tqdm
from warcio import WARCWriter, StatusAndHeaders
from warcio.warcwriter import BufferWARCWriter
from warcio.recordbuilder import RecordBuilder
from warcio.recordloader import ArcWarcRecord
import web_monitoring_db


logger = logging.getLogger()

# Headers known to have been generated by Versionista, and not sourced from the
# original captured request.
BAD_HEADERS = set([
    'age', 'date', 'vary', 'expires', 'x-cachee', 'connection', 'accept-ranges', 'cache-control', 'transfer-encoding'
])

GIGABYTE = 1024 * 1024 * 1024

WARC_VERSION = '1.1'


class BadDataError(Exception):
    reason = 'Bad_data'
    version_id = ''

    def __init__(self, version_id, message=None, reason=None):
        self.version_id = version_id
        self.reason = reason or self.reason
        super().__init__(f'{message or self.reason} (version={version_id})')


class MissingBodyError(BadDataError):
    reason = 'Body_never_saved'


# Based on warcio's implementation:
# https://github.com/webrecorder/warcio/blob/6775fb9ea3505db144a145c5a8b3ba1dfb822ac1/warcio/recordbuilder.py#L46-L52
# This is... not really up-to-spec, but generally good enough.
def serialize_warc_fields(fields_dict: dict) -> BytesIO:
    output = BytesIO()
    for name, value in fields_dict.items():
        if not value:
            continue

        line = f'{name}: {value}\r\n'
        output.write(line.encode('utf-8'))

    return output


def create_metadata_record(writer: RecordBuilder, uri: str, header: dict, data: dict) -> ArcWarcRecord:
    payload = serialize_warc_fields(data)
    length = payload.tell()
    payload.seek(0)
    return writer.create_warc_record(
        uri,
        'metadata',
        warc_headers_dict=header,
        payload=payload,
        length=length
    )


def format_datetime_http(time):
    # email.utils does not support the UTC object dateutil uses, so fix it.
    return email.utils.format_datetime(time.astimezone(timezone.utc), usegmt=True)


def format_datetime_iso(time):
    iso_time = time.isoformat()
    # Use compact representation for UTC
    if iso_time.endswith('+00:00'):
        no_tz_date = iso_time.split("+", 1)[0]
        iso_time = f'{no_tz_date}Z'
    return iso_time


def status_text(code):
    status = HTTPStatus(code)
    return f'{status.value} {status.phrase}'


def load_response_body(version):
    body_response = httpx.get(version['body_url'])
    if body_response.status_code == 404:
        raise MissingBodyError(version['uuid'])

    actual_hash = hashlib.sha256(body_response.content).hexdigest()
    if actual_hash != version['body_hash']:
        detail = f'  Expected: {version["body_hash"]}\n  Actual:   {actual_hash}'
        raise AssertionError(f'Saved body does not match expected hash for version {version["uuid"]}\n{detail}')

    return body_response.content


class WarcSeries:
    def __init__(self, base_path, gzip=True, size=8 * GIGABYTE, info=None, revisit_cache_size=10_000):
        self._file: BufferedWriter | None = None
        self._writer: WARCWriter | None = None
        self._created_names = Counter()
        self._revisit_cache = OrderedDict()
        self._revisit_cache_size = revisit_cache_size
        self.size: int = size
        self.gzip: bool = gzip
        self.warcinfo: dict = info or {}

        path = Path(base_path)
        if path.is_dir():
            self.directory = path
            self.file_base = 'archive'
        else:
            self.directory = path.parent
            self.file_base = path.name

    def close(self):
        self._close_writer()

    def write_records(self, records):
        writer = self._writer
        if not writer:
            record_time = parse_timestamp(records[0].rec_headers.get_header('WARC-Date'))
            writer = self._create_writer(record_time.strftime('--%Y-%m-%dT%H%M%S'))

        for record in records:
            writer.write_record(record)

        if self._file and self._file.tell() > self.size:
            self._close_writer()

    def cache_revisitable_record(self, record, key):
        if len(self._revisit_cache) >= self._revisit_cache_size:
            self._revisit_cache.popitem()

        headers = record.rec_headers
        self._revisit_cache[key] = {
            'id': headers.get_header('WARC-Record-ID'),
            'warc_digest': headers.get_header('WARC-Payload-Digest'),
            'uri': headers.get_header('WARC-Target-URI'),
            'date': headers.get_header('WARC-Date'),
        }

    def get_revisit(self, key) -> dict | None:
        return self._revisit_cache.get(key)

    @property
    def builder(self) -> RecordBuilder:
        if self._writer:
            return self._writer
        else:
            return BufferWARCWriter(warc_version=WARC_VERSION)

    def _close_writer(self) -> None:
        self._revisit_cache.clear()
        self._writer = None
        if self._file:
            self._file.close()
            self._file = None

    def _create_writer(self, suffix='') -> WARCWriter:
        self._close_writer()

        base_name = self.file_base + suffix
        self._created_names[base_name] += 1
        if self._created_names[base_name] > 1:
            base_name += f'-{self._created_names[base_name]}'

        file_name = f'{base_name}.warc'
        if self.gzip:
            file_name += '.gz'

        print(f'OPENING WARC: "{self.directory / file_name}"')
        self.directory.mkdir(parents=True, exist_ok=True)
        self._file = open(self.directory / file_name, 'wb')
        self._writer = WARCWriter(self._file, gzip=self.gzip, warc_version=WARC_VERSION)

        self._writer.write_record(self._writer.create_warcinfo_record(file_name, {
            'software': f'warcio/{importlib.metadata.version("warcio")}',
            'format': f'WARC file version {WARC_VERSION}',
            **self.warcinfo
        }))

        return self._writer

    def __enter__(self) -> Self:
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()


def create_version_records(warc: WarcSeries, version: dict) -> list[ArcWarcRecord]:
    records = []
    version_id = version['uuid']
    capture_time = version["capture_time"]

    if version['status'] is None:
        raise BadDataError(version_id, reason='Missing_status')

    if version['body_url'] is None:
        raise MissingBodyError(version_id)

    history = [version['url']]
    if version['source_metadata'].get('redirects'):
        history.extend(version['source_metadata']['redirects'])

    first_record_id = None
    previous_url = None
    final_url = history[-1]
    for index, url in enumerate(history):
        database_url = f'https://api.monitoring.envirodatagov.org/api/v0/versions/{version_id}'
        record_id = f'<{database_url}/responses/{index}>'
        warc_header = {
            'WARC-Record-ID': record_id,
            'WARC-Date': format_datetime_iso(capture_time),
            # This field is non-standard, and comes from warcit:
            #   https://github.com/webrecorder/warcit#warc-structure-and-format
            # We put the Versionista URL here and the WM database URL in the
            # metadata record.
            'WARC-Source-URI': version['source_metadata']['url'],
        }
        if index == 0:
            first_record_id = record_id
        else:
            warc_header['WARC-Concurrent-To'] = first_record_id

        recorded_headers = { 'Date': format_datetime_http(capture_time) }
        if url == final_url:
            if version['media_type']:
                recorded_headers['Content-Type'] = version['media_type']
            if version['headers']:
                for key, value in version['headers'].items():
                    if key.lower() not in BAD_HEADERS:
                        recorded_headers[key] = value
            http_headers = StatusAndHeaders(status_text(version['status']), recorded_headers.items(), protocol='HTTP/1.1')

            revisit = warc.get_revisit(version['body_hash'])
            if revisit:
                # For some reason this is not an option on create_revisit_record.
                warc_header['WARC-Refers-To'] = revisit['id']
                records.append(warc.builder.create_revisit_record(
                    url,
                    revisit['warc_digest'],
                    revisit['uri'],
                    revisit['date'],
                    http_headers=http_headers,
                    warc_headers_dict=warc_header
                ))
            else:
                record = warc.builder.create_warc_record(
                    url,
                    'response',
                    payload=BytesIO(load_response_body(version)),
                    http_headers=http_headers,
                    warc_headers_dict=warc_header
                )
                records.append(record)
                warc.cache_revisitable_record(record, version['body_hash'])
        else:
            recorded_headers['Location'] = history[index + 1]
            http_headers = StatusAndHeaders(status_text(302), recorded_headers.items(), protocol='HTTP/1.1')
            records.append(warc.builder.create_warc_record(
                url,
                'response',
                payload=None,
                http_headers=http_headers,
                warc_headers_dict=warc_header
            ))

        if not first_record_id:
            first_record_id = record_id

        records.append(create_metadata_record(
            warc.builder,
            url,
            header={
                'WARC-Date': format_datetime_iso(capture_time),
                'WARC-Refers-To': record_id,
                'WARC-Concurrent-To': first_record_id,
            },
            data={
                # Directly listed in WARC standard:
                'via': previous_url,
                'hopsFromSeed': 'R' * index,
                # Standardized via Dublin Core:
                'title': version['title'],
                'source': database_url
            }
        ))

        previous_url = url

    return records


def main(*, start=0, limit=0, name='versionista', gzip=True, warc_size=int(7.95 * GIGABYTE)):
    logging.basicConfig(level=logging.WARNING)

    # The magic number here is the current count of Versionista records.
    expected_records = min(845_325, limit)

    chunk_size = min(1000, limit)
    db_client = web_monitoring_db.Client.from_env()

    skipped = Counter()

    warc_builder = WarcSeries(name, gzip=gzip, size=warc_size, info={
        'operator': '"Environmental Data & Governance Initiative" <contact@envirodatagov.org>',
        'description': dedent("""\
            Web content captured by EDGI's Web Monitoring project using
            Versionista (https://versionista.com). This WARC is synthesized
            from data that was originally archived extracted from
            Versionista via https://github.com/edgi-govdata-archiving/versionista-outputter
            and https://github.com/edgi-govdata-archiving/web-monitoring-versionista-scraper.""").replace('\n', ' ')
    })

    try:
        with warc_builder as warc:
            versions = db_client.get_versions(source_type='versionista', chunk_size=chunk_size)
            if limit:
                versions = islice(versions, start, limit)

            progress_bar = tqdm(versions, unit=' versions', total=expected_records)
            for version in progress_bar:
                try:
                    warc.write_records(create_version_records(warc, version))
                except BadDataError as error:
                    progress_bar.write(f'WARNING: {error}')
                    skipped[error.reason] += 1
                except Exception as error:
                    raise RuntimeError(f'Error processing version {version.get("uuid")}: {error}') from error

        # FIXME: Add resource record with logs:
        # https://iipc.github.io/warc-specifications/guidelines/warc-implementation-guidelines/#use-of-resource-records-for-processing-information

    finally:
        print(f'Skipped {skipped.total()} Versionista versions:')
        for reason, count in skipped.items():
            print(f'  {reason.ljust(25, ".")} {str(count).rjust(5)}')


if __name__ == '__main__':
    main(limit=100, name='./out/edgi_wm_versionista', gzip=False)
